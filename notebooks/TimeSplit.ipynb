{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import datetime\n",
    "import sklearn\n",
    "import sqlalchemy as sa\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.base import BaseEstimator\n",
    "from tqdm import trange\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (39) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n",
      "/home/alex/anaconda3/lib/python3.6/site-packages/numpy/lib/arraysetops.py:472: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "trans_feat = pd.read_csv('../data/data1per.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class forget_model(BaseEstimator):\n",
    "    def __init__(self, model=LogisticRegression(),n_splits=4, balance=None):\n",
    "        self.models = [] # лист, в котором будут храниться обученные модели для каждого сплита\n",
    "        self.n_splits = n_splits # число сплитов\n",
    "        self.balance = balance # должен поддерживать большинство \n",
    "        # методов из imbalanced-learn, которые поддерживают fit_sample\n",
    "        self.model = model # модели, поддерживающие fit, predict и predict_proba\n",
    "    \n",
    "    # inputs - уникальные значения дат\n",
    "    def _iterate_megabatches(self, inputs, n_splits): # делает разбиение по времени без пересечений\n",
    "        batchsize = len(inputs)//n_splits\n",
    "        for start_idx in trange(0, len(inputs) - batchsize + 1, batchsize):\n",
    "            excerpt = [start_idx, start_idx + batchsize-1]\n",
    "            yield inputs[excerpt]\n",
    "    \n",
    "    # X - это весь датафрейм за два месяца ВМЕСТЕ с таргетом\n",
    "    def fit(self, X):\n",
    "        models = [] # пустой список моделей\n",
    "        batch_df_i_neg = pd.DataFrame() # заготовки для не фрода в итом сплите\n",
    "        batch_df_pos = pd.DataFrame() #  заготовка для фрода\n",
    "        unique_TS_split = np.array(sorted(X.TS_indexer.unique())) # получаем число уникальных дат\n",
    "        \n",
    "        for train_index in self._iterate_megabatches(inputs=unique_TS_split, n_splits=self.n_splits): \n",
    "            batch_df_i_old_neg = batch_df_i_neg # сохраняем прошлуб базу не фрода\n",
    "            batch_df_i = X[X.TS_indexer.isin(train_index)] # получаем базу данных итого сплита\n",
    "            batch_df_i_pos = batch_df_i[batch_df_i.label==1] #  разбиваем ее на фрод\n",
    "            batch_df_i_neg = batch_df_i[batch_df_i.label==0] #  и не фрод\n",
    "            batch_df_pos = pd.concat([batch_df_pos, batch_df_i_pos]) # получаем данные не фрода для обучения\n",
    "            batch_df_neg = pd.concat([batch_df_i_old_neg, batch_df_i_neg]) # получаем данные фрода для обучения\n",
    "            df_i = pd.concat([batch_df_pos, batch_df_neg]) #  объединяем фрод и не фрод\n",
    "            X_df_i = df_i.drop(['label', 'short_date','TS_indexer', 'user_id'], axis=1) #  выделяем обучение\n",
    "            y_df_i = df_i.label #  и таргет\n",
    "            if self.balance is None: #  без балансировки просто продолжаем\n",
    "                pass\n",
    "            else: \n",
    "                balancing = self.balance \n",
    "                X_df_balanced_i, y_df_balanced_i = balancing.fit_sample(X_df_i, y_df_i) # делаем перебалансировку\n",
    "                X_df_i = X_df_balanced_i\n",
    "                y_df_i = y_df_balanced_i\n",
    "\n",
    "            model_i = self.model # итая модель\n",
    "            model_i.fit(X_df_i, y_df_i) # обучаем итую модель\n",
    "\n",
    "            \n",
    "            models.append(model_i) #  сохраняем ее в лист\n",
    "        self.models = models #  возвращаем в селф\n",
    "\n",
    "\n",
    "    def predict(self, X_test): \n",
    "        pr = np.zeros(len(X_test)) # заготовка\n",
    "        for model_i in self.models:\n",
    "            pr_i = model_i.predict(X_test) # предсказания итой модели\n",
    "            pr = np.vstack([pr,pr_i]) # объединяем предсказания\n",
    "        predictions = np.array([Counter(x).most_common(1)[0][0] for x in pr[1:].T]) #  простое голосвание\n",
    "        return predictions\n",
    "    \n",
    "    def predict_proba(self, X_test):\n",
    "        pr = np.array([np.zeros(len(X_test))]*2).T #заготовка\n",
    "        for model_i in self.models:\n",
    "            pr_i = model_i.predict_proba(X_test) #  получаем вероятности итой модели\n",
    "            pr = np.sum([pr, pr_i], axis = 0) \n",
    "        predictions = pr/len(self.models) #  получаем усредненные вероятности\n",
    "        return predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alex/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py:2717: DtypeWarning: Columns (2) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "trans_feat = pd.read_csv('../data/frst_chunk.csv', index_col=0)\n",
    "trans_feat['TS_indexer'] = trans_feat.short_date\n",
    "trans_feat.fillna(0, inplace=True)\n",
    "trans_feat.drop(['event_id', 'event_time'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = forget_model(model=LogisticRegression(), n_splits=4, balance=SMOTETomek())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  4.56it/s]\n"
     ]
    }
   ],
   "source": [
    "m.fit(trans_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = m.predict(trans_feat.drop(['label', 'short_date','TS_indexer', 'user_id'], axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2616.0"
      ]
     },
     "execution_count": 376,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 377,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((res == trans_feat.label)[np.argwhere(trans_feat.label == 1).ravel()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_val_ts_split(X, th=0.7, split_target=None): # X - датафрейм, в котором есть  TS_indexer\n",
    "    unique_TS_split = np.array(sorted(X.TS_indexer.unique()))\n",
    "    batchsize = int(len(unique_TS_split)*th)\n",
    "    excerpt_train = [0, batchsize-1]\n",
    "    excerpt_val = [batchsize, len(unique_TS_split)-1]\n",
    "    d_train =  X[X.TS_indexer.isin(unique_TS_split[excerpt_train])]\n",
    "    d_val = X[X.TS_indexer.isin(unique_TS_split[excerpt_val])]\n",
    "    if split_target is None:\n",
    "        return d_train, d_val\n",
    "    else:\n",
    "        return d_train.drop('label', axis=1), d_train.label, d_val.drop('label', axis=1), d_val.label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = trans_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x1, y1, x2, y2 = train_val_ts_split(X , th=0.7, split_target=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1, X2 = train_val_ts_split(X , th=0.7, split_target=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# внутри датасетов данные не сортированы по времени, но находятся в рамках диапазано."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
