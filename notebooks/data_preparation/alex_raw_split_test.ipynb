{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "в данном ноутбуке мы разбиваем обучающую выборку на много частей чтобы потом было проще загружать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import datetime\n",
    "import sklearn\n",
    "import sqlalchemy as sa\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание коннекта к БД и крурсора для запросов\n",
    "conn = sqlite3.connect('../data/input/user_info_HSE_hashed.sqlite')\n",
    "c = conn.cursor()\n",
    "\n",
    "# создание engine для чтения данных в DataFrame\n",
    "engine = sa.create_engine('sqlite:///../data/input/user_info_HSE_hashed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Перечень таблиц: [('rsa_event_log',)]\r\n",
      "\r\n",
      "\n",
      "Столбцы в таблице: ['cdf_s_123', 'cdf_s_124', 'cdf_s_127', 'cdf_s_135', 'cdf_s_130', 'cdf_s_129', 'cdf_s_134', 'cdf_s_128', 'cdf_s_138', 'cdf_s_126', 'cdf_s_133', 'cdf_s_136', 'cdf_s_137', 'cdf_s_140', 'cdf_s_178_hashed', 'cdf_s_19', 'cdf_s_20', 'cdf_s_218', 'cdf_s_294', 'cdf_s_299', 'amount_original', 'channel_indicator_desc', 'data_i_118', 'data_i_119', 'data_i_120', 'data_i_154', 'data_s_65', 'event_description', 'event_id', 'event_time', 'ext_acct_number_hashed', 'hardwareid', 'short_date', 'user_acct_number_hashed', 'user_agent_string_hash', 'browser_plugins_hash', 'screen_hash', 'user_id', 'ip_address', 'ip_country', 'ip_region', 'ip_city', 'ip_isp']\n"
     ]
    }
   ],
   "source": [
    "# пример получения перечня таблиц (в данном случае она будет 1)\n",
    "c.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print ('Перечень таблиц: {0}\\r\\n\\r\\n'.format(c.fetchall()))\n",
    "\n",
    "# перечень столбцов\n",
    "c.execute(\"SELECT * FROM rsa_event_log LIMIT 1;\")\n",
    "cols = [descr[0] for descr in c.description]\n",
    "print ('Столбцы в таблице: {0}'.format(cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Пример форомирования DataFrame из БД с заполнением типа полей\n",
    "\n",
    "def transform_cols (df, dict_col_types = None):\n",
    "    # Расширяйте для необходимых столбцов и их явной типизации\n",
    "    if dict_col_types is None:\n",
    "        dict_col_types = {\n",
    "        'amount_original':(float, 0.0),\n",
    "        'cdf_s_126':(str, u'null'),\n",
    "        'cdf_s_138':(str, u'null'),\n",
    "        'channel_indicator':(str, u'null'),\n",
    "        'event_description':(str, u'null'),\n",
    "        'cdf_s_294':(int, 0),\n",
    "        'cdf_s_140':(float, 0.0),\n",
    "        'data_i_120':(int, 0),\n",
    "        'cdf_s_218':(str, u'null'),\n",
    "        'data_s_65':(int, 0),\n",
    "        'cdf_s_127':(int, 30),\n",
    "        'cdf_s_135':(int, 30),\n",
    "        'cdf_s_130':(int, 30),\n",
    "        'cdf_s_129':(int, 30),\n",
    "        'cdf_s_134':(int, 30),\n",
    "        'data_i_154':(float, np.nan),\n",
    "        'cdf_s_133':(int, 30),\n",
    "        'cdf_s_20':(str, u'null'),\n",
    "        'cdf_s_299':(str, u'null'),\n",
    "        }\n",
    "                \n",
    "    if df.shape[0] > 0:\n",
    "        df.replace(u'null', np.nan, inplace=True)\n",
    "\n",
    "        for i in dict_col_types:\n",
    "            if i in df.columns:\n",
    "                df[i] = df[i].fillna(dict_col_types[i][1]).astype(dict_col_types[i][0])\n",
    "    \n",
    "    return df\n",
    "    \n",
    "\n",
    "def df_from_sql (sql, parse_dates = ['event_time','cdf_s_19', 'cdf_s_123','cdf_s_124'], dict_col_types = None, chunk_size = None, engine=engine):\n",
    "    \"\"\"\n",
    "    Функция вощвращающая df в результате переданного sql-запроса\n",
    "    Также поддерживается приведение столбцов к заданным типам \n",
    "    и чтение по блокам\n",
    "    \"\"\"\n",
    "     \n",
    "    if chunk_size is not None:\n",
    "        df_iter =  [transform_cols(chunk, dict_col_types) for chunk in pd.read_sql_query(sql, engine, parse_dates = parse_dates, chunksize=chunk_size)]\n",
    "    else:\n",
    "        df_iter = transform_cols(pd.read_sql_query(sql, engine, parse_dates = parse_dates), dict_col_types)\n",
    "        #df_iter = pd.read_sql_query(sql, engine, con=conn,  parse_dates = parse_dates)\n",
    "    \n",
    "    return df_iter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_users = df_from_sql('select DISTINCT user_id from rsa_event_log;',\n",
    "                        engine=conn,\n",
    "                        parse_dates=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(seed = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_users_list = sorted(all_users['user_id'].astype(str).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rng.shuffle(all_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MBK14703099', '4196086', '58804548', '32732927', '135593']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_users_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def chunks(l, n):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(l), n):\n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85764"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_users_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_ids_chunks = list(chunks(all_users_list, len(all_users_list) // 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('../data/raw_splits/test/user_ids_chunks.pkl', 'wb') as f:\n",
    "    pickle.dump(user_ids_chunks, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "folder = '../data/raw_splits/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41111bbe311549d2b3b583e2e2559645"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for n, chunk in tqdm_notebook(list(enumerate(user_ids_chunks))):\n",
    "    sql_query_str = \"select * from rsa_event_log where user_id in ({0});\".format(str(chunk)[1:-1])\n",
    "    data = pd.read_sql_query(sql_query_str, con=conn, \n",
    "                             parse_dates = ['event_time','cdf_s_19', 'cdf_s_123','cdf_s_124'])\n",
    "    fname = os.path.join(folder, f'chunk_{n}.fth')\n",
    "    data.to_feather(fname)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
